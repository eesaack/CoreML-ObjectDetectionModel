{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this practical work, we implement the Soft Actor-Critic (SAC) algorithm to train an agent in the Inverted Pendulum environment. SAC is an off-policy reinforcement learning method designed for continuous action spaces. It improves exploration by maximizing both reward and policy entropy, leading to more stable learning.\n",
        "\n",
        "The Inverted Pendulum is a classic control problem where the goal is to balance a pole on a moving cart using continuous control inputs. You will complete key functions in the SAC implementation, gaining hands-on experience with policy updates, Q-learning, and entropy-based exploration.\n",
        "\n",
        "For more details on SAC, refer to:\n",
        "[ðŸ”— Spinning Up: SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html)"
      ],
      "metadata": {
        "id": "e0Jwh_DfJrp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful installs and imports"
      ],
      "metadata": {
        "id": "ie0IMAUkcoQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium[mujoco]"
      ],
      "metadata": {
        "id": "rZjPoXiSEPGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stable_baselines3"
      ],
      "metadata": {
        "id": "oQoaz6XkF0RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQPo6QcQaDlB"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from google.colab import output\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import gymnasium as gym\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful function"
      ],
      "metadata": {
        "id": "xDiw_sj5ctaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(env_id, seed, idx):\n",
        "    def thunk():\n",
        "        env = gym.make(env_id)\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        env.action_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk"
      ],
      "metadata": {
        "id": "_IEUkaRJKiaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the actor-critic models"
      ],
      "metadata": {
        "id": "-TWsZG4EJ256"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `SoftQNetwork` class is a neural network used to estimate the Q-value (action-value) function in SAC. It takes both the state `x` and action `a` as input, processes them through the network. In the `SoftQNetwork` class, complete the `forward` function to compute the Q-value for a given state-action pair. We use Relu function as activation."
      ],
      "metadata": {
        "id": "t-9fBScbLQ_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftQNetwork(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(\n",
        "            np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape),\n",
        "            256,\n",
        "        )\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        <YOUR CODE HERE>\n",
        "        return x"
      ],
      "metadata": {
        "id": "XabenXamacTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many reinforcement learning environments, the **action space** is bounded, meaning that the actions the agent can take are constrained within a specific range, such as between -1 and 1. However, the **actor network** may output actions that are not within this range, which can cause problems when interacting with the environment.\n",
        "\n",
        "To handle this, we use **rescaling**. The idea is to map the output of the actor network, which may be unconstrained (i.e., can range from negative to positive infinity), into the valid action range for the environment.\n",
        "\n",
        "How rescaling works:\n",
        "\n",
        "- **`action_scale`**: This is the factor by which we multiply the output of the network to scale the actions back into the environment's action space.\n",
        "- **`action_bias`**: This shifts the scaled actions to ensure they are centered around the middle of the action bounds.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fi1S-FeBMlt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `Actor` class, complete the `forward` and `get_action` methods.\n"
      ],
      "metadata": {
        "id": "tQtuvHKRNp0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_STD_MAX = 2\n",
        "LOG_STD_MIN = -5\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
        "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
        "        # action rescaling\n",
        "        self.register_buffer(\n",
        "            \"action_scale\",\n",
        "            torch.tensor(\n",
        "                (env.single_action_space.high - env.single_action_space.low) / 2.0,\n",
        "                dtype=torch.float32,\n",
        "            ),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"action_bias\",\n",
        "            torch.tensor(\n",
        "                (env.single_action_space.high + env.single_action_space.low) / 2.0,\n",
        "                dtype=torch.float32,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        <YOUR CODE HERE>\n",
        "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp\n",
        "\n",
        "        return mean, log_std\n",
        "\n",
        "    def get_action(self, x):\n",
        "        mean, log_std = <YOUR CODE HERE>\n",
        "        std = <YOUR CODE HERE>\n",
        "        normal = torch.distributions.Normal(mean, std)\n",
        "        x_t = <YOUR CODE HERE> # Sample x_t for the normal distribution N(mean,std)\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        # Enforcing Action Bound on log probability and mean\n",
        "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
        "        return action, log_prob, mean"
      ],
      "metadata": {
        "id": "ASBOpDjaagvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the agent"
      ],
      "metadata": {
        "id": "Swoh4DynIKCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "total_timesteps = 30000\n",
        "buffer_size = int(1e6)\n",
        "gamma = 0.99\n",
        "tau = 0.005\n",
        "learning_starts = 5000\n",
        "batch_size = 256\n",
        "policy_lr = 3e-4\n",
        "q_lr = 1e-3\n",
        "policy_frequency = 2\n",
        "target_network_frequency = 1\n",
        "alpha = 0.2\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "S9M3sZt_e38c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entropy Adjustment in SAC**\n",
        "\n",
        "In **Soft Actor-Critic (SAC)**, entropy is used to encourage exploration, and its level is dynamically adjusted. The **target entropy** is computed as `target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()`, which is the negative of the number of action dimensions, ensuring the policy maintains a certain level of randomness. The temperature parameter `alpha` controls the trade-off between maximizing rewards and maintaining entropy. Instead of using a fixed value, **SAC learns alpha dynamically**. To achieve this, `log_alpha = torch.zeros(1, requires_grad=True, device=device)` initializes `log_alpha` as a trainable parameter. The actual alpha value is then computed as `alpha = log_alpha.exp().item()`, ensuring it remains positive. Finally, `a_optimizer = optim.Adam([log_alpha], lr=q_lr)` sets up an Adam optimizer to update `log_alpha` during training, allowing the agent to automatically tune its level of exploration and adapt to the environment.\n"
      ],
      "metadata": {
        "id": "U1yjJKcyOZnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()\n",
        "log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
        "alpha = log_alpha.exp().item()\n",
        "a_optimizer = optim.Adam([log_alpha], lr=q_lr)"
      ],
      "metadata": {
        "id": "bChhaZ5_eP7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the different actor and critic networks and their optimizers of the agent."
      ],
      "metadata": {
        "id": "g0yDdE-YO4an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actor = Actor(envs).to(device)\n",
        "qf1 = SoftQNetwork(envs).to(device)\n",
        "qf2 = SoftQNetwork(envs).to(device)\n",
        "qf1_target = SoftQNetwork(envs).to(device)\n",
        "qf2_target = SoftQNetwork(envs).to(device)\n",
        "qf1_target.load_state_dict(qf1.state_dict())\n",
        "qf2_target.load_state_dict(qf2.state_dict())\n",
        "q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=q_lr)\n",
        "actor_optimizer = optim.Adam(list(actor.parameters()), lr=policy_lr)"
      ],
      "metadata": {
        "id": "RR7b1Trvaz7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "\n",
        "rb = ReplayBuffer(\n",
        "        buffer_size,\n",
        "        envs.single_observation_space,\n",
        "        envs.single_action_space,\n",
        "        device,\n",
        "        n_envs=num_envs,\n",
        "        handle_timeout_termination=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "iZKdA8aRhD3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For reproductibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "l7KodFdGjalr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code implements the **Soft Actor-Critic (SAC) training loop** for the **Inverted Pendulum** environment.\n",
        "\n",
        "1. **Action Selection:**  \n",
        "   - Complete the line `actions, _, _ = <YOUR CODE HERE>` to ensure actions are chosen by the **Actor network** after the initial exploration phase.  \n",
        "   \n",
        "2. **Target Q-Value Computation:**  \n",
        "   - Fill in `next_state_actions, next_state_log_pi, _ = <YOUR CODE HERE>` to obtain the next stateâ€™s action and log probability.  \n",
        "   - Implement `qf1_next_target`, `qf2_next_target`, and `min_qf_next_target`, ensuring you **include the entropy term** when computing `next_q_value`.  \n",
        "\n",
        "3. **Critic Loss Calculation:**  \n",
        "   - Complete `<YOUR CODE HERE>` to compute `qf1_loss`, `qf2_loss`, and `qf_loss` using **Mean Squared Error (MSE)** between the predicted Q-values and `next_q_value`.  \n",
        "   - Optimize the critic networks accordingly.  \n",
        "\n",
        "4. **Policy Update:**  \n",
        "   - Implement `pi, log_pi, _ = <YOUR CODE HERE>` to sample actions from the policy.  \n",
        "   - Compute `min_qf_pi` and `actor_loss`, ensuring the **entropy term (Î± * log_pi)** is included.  \n",
        "\n",
        "5. **Alpha Update:**  \n",
        "   - Complete the code to update `log_alpha` using the **alpha loss** equation:  \n",
        "\n",
        "\n",
        "6. **Target Networks Update:**  \n",
        "   - Explain the purpose of the **target network update step** and how the soft update formula helps stabilize training.\n"
      ],
      "metadata": {
        "id": "B5MpRt7sQ6jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envs = gym.vector.SyncVectorEnv(\n",
        "        [make_env(\"InvertedPendulum-v4\", i, i) for i in range(num_envs)]\n",
        "    )\n",
        "envs.single_observation_space.dtype = np.float32\n",
        "\n",
        "episodic_returns = []\n",
        "episode_steps = []\n",
        "\n",
        "obs, _ = envs.reset(seed=seed)\n",
        "\n",
        "for global_step in range(total_timesteps):\n",
        "    if global_step < learning_starts:\n",
        "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
        "    else:\n",
        "        actions, _, _ = <YOUR CODE HERE> # Use the actions selected by the Actor\n",
        "        actions = actions.detach().cpu().numpy()\n",
        "\n",
        "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
        "\n",
        "    if \"episode\" in infos:\n",
        "      episodic_returns.append(infos['episode']['r'])\n",
        "      epiosde_step.append(global_step)\n",
        "      print(f\"global_step={global_step}, episodic_return={infos['episode']['r']}\")\n",
        "\n",
        "    # Add transition to replay buffer\n",
        "    rb.add(obs, next_obs.copy(), actions, rewards, terminations, infos)\n",
        "\n",
        "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
        "    obs = next_obs\n",
        "\n",
        "    # ALGO LOGIC: training.\n",
        "    if global_step > learning_starts:\n",
        "        data = rb.sample(batch_size)\n",
        "        with torch.no_grad():\n",
        "            next_state_actions, next_state_log_pi, _ = <YOUR CODE HERE>\n",
        "            qf1_next_target = <YOUR CODE HERE>\n",
        "            qf2_next_target = <YOUR CODE HERE>\n",
        "            min_qf_next_target = <YOUR CODE HERE> # DO NOT FORGET THE ENTROPY TERM\n",
        "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * gamma * (min_qf_next_target).view(-1)\n",
        "\n",
        "        <YOUR CODE HERE>\n",
        "        qf1_loss = <YOUR CODE HERE>\n",
        "        qf2_loss = <YOUR CODE HERE>\n",
        "        qf_loss = qf1_loss + qf2_loss\n",
        "\n",
        "        # optimize the critics\n",
        "        <YOUR CODE HERE>\n",
        "\n",
        "        if global_step % policy_frequency == 0:\n",
        "            for _ in range(policy_frequency):\n",
        "                pi, log_pi, _ = <YOUR CODE HERE>\n",
        "                qf1_pi = <YOUR CODE HERE>\n",
        "                qf2_pi = <YOUR CODE HERE>\n",
        "                min_qf_pi = <YOUR CODE HERE>\n",
        "                actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
        "\n",
        "                actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                actor_optimizer.step()\n",
        "\n",
        "                # Update alpha\n",
        "                with torch.no_grad():\n",
        "                    _, log_pi, _ = actor.get_action(data.observations)\n",
        "                alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()\n",
        "\n",
        "                <YOUR CODE HERE>\n",
        "                alpha = log_alpha.exp().item()\n",
        "\n",
        "        # update the target networks\n",
        "        if global_step % target_network_frequency == 0:\n",
        "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "            for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
      ],
      "metadata": {
        "id": "ZOkr9dmWeUzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet below generates a plot to visualize the agentâ€™s performance during training. Analyze the plot after running your training and provide a short interpretation of the results."
      ],
      "metadata": {
        "id": "hAzQ3xd6RxfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(episodic_returns)\n",
        "plt.xlabel(\"Training episode\")\n",
        "plt.ylabel(\"Episodic return\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YeB35MR2WByF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the actor"
      ],
      "metadata": {
        "id": "PrlWhBcTXsba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `test_actor` is designed to evaluate a trained SAC policy by running it in the **Inverted Pendulum** environment for `num_episodes` and recording the episodic returns. Complete the missing code, run the function with a trained actor, and interpret the results.\n"
      ],
      "metadata": {
        "id": "UW24EZJ6R2rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_actor(actor, num_episodes=10):\n",
        "  episodic_returns = np.zeros(num_episodes)\n",
        "\n",
        "  for i in range(num_episodes):\n",
        "    episode_return = 0\n",
        "    envs = gym.vector.SyncVectorEnv(\n",
        "        [make_env(\"InvertedPendulum-v4\", i, i, capture_video=False) for i in range(num_envs)]\n",
        "    )\n",
        "    envs.single_observation_space.dtype = np.float32\n",
        "    obs, _ = envs.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      <YOUR CODE HERE>\n",
        "      done = terminated or truncated\n",
        "\n",
        "    episodic_returns[i] = ep_return\n",
        "    print(f\"Episode {i+1}: Return = {ep_return}\")\n",
        "  return {\"mean return\":episodic_returns.mean(), \"std return\":episodic_returns.std()}"
      ],
      "metadata": {
        "id": "iKpMwi2OWbd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_actor(actor)"
      ],
      "metadata": {
        "id": "TWa0uMXCXW1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}